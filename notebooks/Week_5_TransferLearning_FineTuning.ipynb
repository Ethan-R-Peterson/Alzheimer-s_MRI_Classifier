{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c049493c",
      "metadata": {
        "id": "c049493c"
      },
      "source": [
        "## Week 5: Fine-tuning VGG"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c964b5c",
      "metadata": {
        "id": "3c964b5c"
      },
      "source": [
        "### 0. Import the necessary libraries and dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dc258f4",
      "metadata": {
        "id": "9dc258f4",
        "outputId": "5eb2b0d8-14fc-45f7-ea74-af6b977df875",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/lukechugh/best-alzheimer-mri-dataset-99-accuracy?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 71.5M/71.5M [00:02<00:00, 31.4MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/lukechugh/best-alzheimer-mri-dataset-99-accuracy/versions/1\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as T\n",
        "from torchvision.models import vgg16, VGG16_Weights\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.optim import lr_scheduler\n",
        "from tqdm import tqdm\n",
        "from torchvision import models\n",
        "from PIL import Image\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "import cv2\n",
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"lukechugh/best-alzheimer-mri-dataset-99-accuracy\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ceb379a",
      "metadata": {
        "id": "6ceb379a"
      },
      "source": [
        "### 1. Data loading and preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87b38100",
      "metadata": {
        "id": "87b38100"
      },
      "source": [
        "#### 1.1 Transformation for training dataset\n",
        "\n",
        "Here, we create a series of transformations for all images in the following sequence:\n",
        "- Converting the images to grayscale with 3 identical channels (to match the expected input shape for ResNet)\n",
        "- Resizing the images to 256 x 256 pixels\n",
        "- Cropping the resized images to 224 x 224 pixels from the center point\n",
        "- Convert the PIL (Pillow) image to a PyTorch tensor\n",
        "- Normalize the tensor with each channel to the respective mean and standard deviation. The specific mean and standard deviation values are calculated from the ImageNet dataset, which ResNet was originally trained on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b087019c",
      "metadata": {
        "id": "b087019c"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement the above series of transformations for training images\n",
        "train_transforms = T.Compose([\n",
        "    T.Grayscale(num_output_channels=3),\n",
        "    T.Resize(256),\n",
        "    T.CenterCrop(224),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.485,0.456,0.406],\n",
        "                         std=[0.229,0.224,0.225]),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fa78b88",
      "metadata": {
        "id": "3fa78b88"
      },
      "source": [
        "#### 1.2 Load and split training dataset\n",
        "\n",
        "We first load the training dataset from the directory. Note: The data is already split into training and testing sets, but we only need to load the training set. Ensure that the root directory of the dataset is `mri_dataset`.\n",
        "\n",
        "While loading the dataset, we also add the transformations declared earlier.\n",
        "\n",
        "Next, we split the loaded data into training and validation sets with an 80-20 split. The validation set is used to evaluate the model during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "617b891a",
      "metadata": {
        "id": "617b891a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c5f2d14-8f7f-4720-e268-18290c51d67f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 8192 | Validation: 2048\n"
          ]
        }
      ],
      "source": [
        "full_dataset = ImageFolder(path + \"/Combined Dataset/train\", transform=train_transforms)\n",
        "\n",
        "train_len = int(0.8 * len(full_dataset))\n",
        "val_len = len(full_dataset) - train_len\n",
        "train_ds, val_ds = random_split(full_dataset, [train_len, val_len])\n",
        "print(f\"Train: {len(train_ds)} | Validation: {len(val_ds)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1023e24e",
      "metadata": {
        "id": "1023e24e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02b5496a-ae6b-4437-dcd8-61bc881210fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=4)\n",
        "val_dl = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba1d6e24",
      "metadata": {
        "id": "ba1d6e24"
      },
      "source": [
        "### 2. VGG fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "986cbdde",
      "metadata": {
        "id": "986cbdde"
      },
      "source": [
        "#### 2.1 Set default weights for the pre-trained VGG 16 model.\n",
        "\n",
        "As mentioned previously, this model was trained on the ImageNet dataset. We will use the weights from this pre-trained model and adjust them to work with our dataset. We are only modifying the final classification block (witth 3 layers) model to have 4 output classes, as opposed to the original 1000."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38d286f3",
      "metadata": {
        "id": "38d286f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14f5e131-b224-407e-ad81-707adf561c0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 528M/528M [00:03<00:00, 165MB/s]\n"
          ]
        }
      ],
      "source": [
        "# TODO: Load VGG model and pre-trained weights\n",
        "weights = VGG16_Weights.DEFAULT\n",
        "model = vgg16(weights=weights)\n",
        "\n",
        "num_classes = 4\n",
        "model.classifier[-1] = nn.Linear(in_features=4096, out_features=num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95f269a0",
      "metadata": {
        "id": "95f269a0"
      },
      "outputs": [],
      "source": [
        "# TODO: Unfreeze final classifier layers\n",
        "for name, param in model.named_parameters():\n",
        "    if \"features.24\" in name or \"features.25\" in name or \\\n",
        "       \"features.26\" in name or \"features.27\" in name or \\\n",
        "       \"features.28\" in name or \"features.29\" in name:\n",
        "        param.requires_grad = True  # unfreeze last conv block\n",
        "    elif \"classifier\" in name:\n",
        "        param.requires_grad = True  # also train classifier\n",
        "    else:\n",
        "        param.requires_grad = False  # keep rest frozen"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3594b8ed",
      "metadata": {
        "id": "3594b8ed"
      },
      "source": [
        "Use the device's GPU if available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15d48bf3",
      "metadata": {
        "id": "15d48bf3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5baa65e2-0bf8-4e7f-8741-ff46dfb9b454"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# Note: If you have a CUDA GPU, change \"mps\" to \"cuda\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else(\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
        "print(device)\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "614c7e53",
      "metadata": {
        "id": "614c7e53"
      },
      "source": [
        "#### 2.2 Declare loss function, optimizer, and scheduler\n",
        "\n",
        "Here, we are using the Cross Entropy Loss function, Adam optimizer, and Learning Rate Scheduler to train the model.\n",
        "\n",
        "- The Cross Entropy Loss function is the best option for our purpuses in fine-tuning the ResNet model, as it expects a softmax output, which the ResNet model outputs.\n",
        "- The Adam optimizer is a good option for fine-tuning purposes and for generalization, which is an important factor for our use-case with medical data. Here, we adjust the learning rates for different layers of the model, increasing as we move forward in the model. This is because the earlier layers pick up higher-level features, while the later layers pick up finer features. Thus, having a lower learning rate for the begining layers helps the model generalize, while a higher learning rate for the later layers helps the model better learn more fine-grained features.\n",
        "- The Learning Rate Scheduler is used to dynamically adjust the learning rate of the optimizer during training, which can help prevent overfitting and improve the model's generalization ability.\n",
        "\n",
        "Before we declare the loss, optimizer, and scheduler, however, we need to balance the class weights due to the significant class imbalance in our dataset. For instance, since the number of images in the 'No impairment' class is significantly higher than the number of images in the 'Moderate impairment' class, we need to assign a higher weight to the latter class to penalize errors in the 'Moderate impairment' class more. This leads the model away from just predicting the 'No impariment' class more often just to score a higher accuracy, and to a more balanced model overall."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82bcd1d8",
      "metadata": {
        "id": "82bcd1d8"
      },
      "outputs": [],
      "source": [
        "# TODO: Extract labels from training data\n",
        "y_train = []\n",
        "for _, labels in train_dl:\n",
        "    y_train.extend(labels.numpy())\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
        "criterion = nn.CrossEntropyLoss(weight=torch.tensor(weights, dtype=torch.float).to(device))\n",
        "\n",
        "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
        "\n",
        "optimizer = torch.optim.Adam(trainable_params, lr=1e-4)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67984543",
      "metadata": {
        "id": "67984543"
      },
      "source": [
        "#### 2.3 Fine-tuning VGG\n",
        "\n",
        "We'll be using 15 epochs for fine-tuning. In each epoch, if the validation accuracy improves from the previous epoch, we'll save the new model weights.\n",
        "\n",
        "As always, we begin with the training part of the loop; moving all the inputs to the GPU (if present), zeroing the optimizer's gradients, calculating the loss, and updating the weights through backpropagation. This is not so different from training traditional CNNs.\n",
        "\n",
        "Moving to the validation phase of each epoch, we set the model to evaluation mode, and we calculate the validation loss and accuracy. This is again similar to training traditional CNNs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bd90bf5",
      "metadata": {
        "id": "3bd90bf5",
        "outputId": "a559e69f-85a5-445e-900e-3e79c9efc347",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 932
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/256 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Training: 100%|██████████| 256/256 [00:58<00:00,  4.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5339, Acc: 0.7604\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 64/64 [00:11<00:00,  5.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.3128, Acc: 0.8701\n",
            "→ Saved best model\n",
            "Epoch 2/15\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 256/256 [00:56<00:00,  4.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2352, Acc: 0.9056\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 64/64 [00:11<00:00,  5.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.2520, Acc: 0.9106\n",
            "→ Saved best model\n",
            "Epoch 3/15\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 256/256 [00:57<00:00,  4.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1019, Acc: 0.9640\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 64/64 [00:10<00:00,  5.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.1856, Acc: 0.9336\n",
            "→ Saved best model\n",
            "Epoch 4/15\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 256/256 [00:57<00:00,  4.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0673, Acc: 0.9769\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 64/64 [00:10<00:00,  5.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.1678, Acc: 0.9482\n",
            "→ Saved best model\n",
            "Epoch 5/15\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 256/256 [00:57<00:00,  4.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0313, Acc: 0.9883\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 64/64 [00:11<00:00,  5.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.1776, Acc: 0.9458\n",
            "Epoch 6/15\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  20%|██        | 52/256 [00:12<00:47,  4.29it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-464658483.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mrunning_corrects\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "best_val_acc = 0.0\n",
        "num_epochs = 15\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(\"-\" * 10)\n",
        "\n",
        "    # --- Train Phase ---\n",
        "    model.train()\n",
        "    running_loss = running_corrects = 0\n",
        "\n",
        "    for inputs, labels in tqdm(train_dl, desc=\"Training\"):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += (preds == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_dl.dataset)\n",
        "    epoch_acc = running_corrects / len(train_dl.dataset)\n",
        "    print(f\"Train Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}\")\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    model.eval()\n",
        "    val_loss = val_corrects = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(val_dl, desc=\"Validating\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "            val_corrects += (preds == labels).sum().item()\n",
        "\n",
        "    val_loss = val_loss / len(val_dl.dataset)\n",
        "    val_acc = val_corrects / len(val_dl.dataset)\n",
        "    print(f\"Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Step LR\n",
        "    scheduler.step()\n",
        "\n",
        "    # Save best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), \"best_vgg_mri.pth\")\n",
        "        print(\"→ Saved best model\")\n",
        "\n",
        "print(\"Training complete. Best Validation Accuracy:\", best_val_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54de4767",
      "metadata": {
        "id": "54de4767"
      },
      "outputs": [],
      "source": [
        "# TODO: Load VGG model with null weights, modify final classifier to have 4 outputs\n",
        "model = models.vgg16(weights=None)\n",
        "\n",
        "model.classifier[6] = nn.Linear(4096, 4)\n",
        "model.load_state_dict(torch.load('best_vgg_mri.pth'))\n",
        "\n",
        "def replace_relu_with_out_of_place(module):\n",
        "    for name, child in module.named_children():\n",
        "        if isinstance(child, torch.nn.ReLU) and child.inplace:\n",
        "            setattr(module, name, torch.nn.ReLU(inplace=False))\n",
        "        else:\n",
        "            replace_relu_with_out_of_place(child)\n",
        "\n",
        "replace_relu_with_out_of_place(model)\n",
        "\n",
        "model.eval().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4955e0a",
      "metadata": {
        "id": "b4955e0a"
      },
      "source": [
        "### 3. Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6657473e",
      "metadata": {
        "id": "6657473e"
      },
      "source": [
        "#### 3.1 Data loading and pre-processing\n",
        "\n",
        "We'll begin by declaring another sequence of pre-processing modifications for the testing data. This is identical to the set of pre-processing modifications we used for the training data.\n",
        "\n",
        "Similar to the training data, we'll load the dataset and loader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c199bcb6",
      "metadata": {
        "id": "c199bcb6"
      },
      "outputs": [],
      "source": [
        "# TODO: Declare pre-processing for testing data\n",
        "preprocess = T.Compose([\n",
        "    T.Grayscale(num_output_channels=3),\n",
        "    T.Resize(256),\n",
        "    T.CenterCrop(224),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.485,0.456,0.406],\n",
        "                         std=[0.229,0.224,0.225]),\n",
        "])\n",
        "dataset = ImageFolder(path + \"/Combined Dataset/test\", transform=preprocess)\n",
        "loader = DataLoader(dataset, batch_size=16, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9b37559",
      "metadata": {
        "id": "f9b37559"
      },
      "source": [
        "#### 3.2 Generate predictions\n",
        "\n",
        "As always, we'll walk through the inputs and labels in the test dataset and append the predictions to the output array called `all_preds`. To be able to evaluate the model's performance, we'll also append the actual labels to another array caled `all_labels`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14765a06",
      "metadata": {
        "id": "14765a06"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in loader:\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = model(inputs)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1417eb8b",
      "metadata": {
        "id": "1417eb8b"
      },
      "source": [
        "#### 3.3 Visualizing Model Performance\n",
        "\n",
        "As with any classification model, we can use the confusion matrix to visualize the performance.\n",
        "\n",
        "It's interesting to note how the model performs on the test set: it's incorrect predictions often do not seem to be too far from the actual values. For instance, for the Mild Impairment label, most of the incorrect predictions are classified as Very Mild Impairment. This makes sense because the labels are on a scale of the severity of impairment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eca0cc0d",
      "metadata": {
        "id": "eca0cc0d"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "disp = ConfusionMatrixDisplay(cm, display_labels=dataset.classes)\n",
        "disp.plot(cmap=\"Blues\", xticks_rotation=\"vertical\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"Classification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=dataset.classes))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "307b3cc1",
      "metadata": {
        "id": "307b3cc1"
      },
      "source": [
        "### 4. Feature visualization (Bonus)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f40e2503",
      "metadata": {
        "id": "f40e2503"
      },
      "source": [
        "Here, we'll take a look at the features learned by the VGG-16 model on a sample image. As a reminder, here's the architecture of the VGG-16 model:\n",
        "\n",
        "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTonMOBS41plEHMuaFXJdfaQ9rwHjofx-Sb8g&s\" width=\"200%\"/>\n",
        "\n",
        "Before we get started, recall that every convolutional layer contains a set of learnable filters (kernels), typically of size n×n (like 3×3).\n",
        "These filters slide over the input image (or feature map), performing dot products to produce activations.\n",
        "The number of filters in a layer determines the number of output channels, and each filter learns to detect specific features.\n",
        "\n",
        "To generate a feature map, we'll use a technique called GradCAM (Gradient-weighted Class Activation Mapping). The core idea of this method is to (1) access the features from a convolutional layer and (2) weight each feature map according to it's importance in predicting the class of the image (hence the 'Class' in GradCAM).\n",
        "\n",
        "The method of calculating the weights is what distinguishes GradCAM from other techniques to generate feature maps.\n",
        "\n",
        "Here's a quick overview of the method that GradCAM uses to calculate the weights:\n",
        "\n",
        "1. Compute the gradients of the class prediction with respect to the activations of the convolutional layer. This provides a measure of how much changing a given element in an activation channel contributes to the prediction of that particular class. The gradients thus also have the same dimensions as the activations.\n",
        "2. Compute the average of the gradients for each channel in the feature map. This provides a single gradient as the overall \"change\" for each channel in the feature map, which are essentially weights for each channel.\n",
        "3. Weight each feature map by the average of the gradients. This provides a single weighted feature map (i.e. activation channel).\n",
        "4. Sum the weighted feature maps together to get the final heatmap. This is effectively the weighted sum of the activations for each channel in the feature map, and is called the GradCAM heatmap.\n",
        "5. Filter out negative values with the ReLU function. This is because negative values don't have a physical interpretation in the context of the model.\n",
        "6. Normalize the heatmap values to be between 0 and 1\n",
        "7. Upscale the heatmap to the same size as the original image.\n",
        "8. Apply the heatmap to the original image and display it!\n",
        "\n",
        "For a more visual explanation, check out this video: https://www.youtube.com/watch?v=_QiebC9WxOc"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a32fe9a8",
      "metadata": {
        "id": "a32fe9a8"
      },
      "source": [
        "#### 4.1 Create forward and backwward hooks\n",
        "\n",
        "A forward hook stores the most recent activaitons in the target layer after a forward pass. A backward hook stores the gradients passed into the target layer after a backward pass (back propagation).\n",
        "\n",
        "The activations provide a set of features, which are areas of importance in the image that are picked up by the layer. The gradients provide a set of weights to highlight the importance of those features. Together, they can be used to create a heatmap of the most important areas in the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5955af2f",
      "metadata": {
        "id": "5955af2f"
      },
      "outputs": [],
      "source": [
        "activations = None\n",
        "gradients = None\n",
        "\n",
        "def forward_hook(module, input, output):\n",
        "    global activations\n",
        "    activations = output.detach()\n",
        "\n",
        "def backward_hook(module, grad_input, grad_output):\n",
        "    global gradients\n",
        "    gradients = grad_output[0].detach()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "186eab7b",
      "metadata": {
        "id": "186eab7b"
      },
      "source": [
        "We will use the last convolutional layer of the fine-tuned VGG16 model to extract the features from the images. Note that this is before the classifier block in the model.\n",
        "\n",
        "The reason for picking the last convolution layer is that it would have picked up the most detailed features from the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72875c61",
      "metadata": {
        "id": "72875c61"
      },
      "outputs": [],
      "source": [
        "target_layer = model.features[29]  # last conv layer in VGG16\n",
        "target_layer.register_forward_hook(forward_hook)\n",
        "target_layer.register_full_backward_hook(backward_hook)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a699240",
      "metadata": {
        "id": "5a699240"
      },
      "source": [
        "#### 4.2 Load and pre-process a sample image\n",
        "\n",
        "We first load a sample image from the test dataset and convert it to RGB. This conversion is important because the model expects the image to have 3 color channels. We run the same preprocessing steps as with the training and testing data for consistency.\n",
        "\n",
        "We then set the requires_grad parameter of the processed image to True to allow the backward hook to properly track the gradients with respect to the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59e56cee",
      "metadata": {
        "id": "59e56cee"
      },
      "outputs": [],
      "source": [
        "img = Image.open(path + \"/Combined Dataset/test/Mild Impairment/1 (2).jpg\")\n",
        "\n",
        "input_tensor = preprocess(img).unsqueeze(0).to(device)\n",
        "\n",
        "input_tensor.requires_grad = True\n",
        "\n",
        "print(\"Input image shape:\", input_tensor.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73a30914",
      "metadata": {
        "id": "73a30914"
      },
      "source": [
        "#### 4.3 Forward and backward passes\n",
        "Next, we perform a forward pass on the processed image and get the output class. This updates the layer's activations in the forward hook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69a4152a",
      "metadata": {
        "id": "69a4152a"
      },
      "outputs": [],
      "source": [
        "output = model(input_tensor)\n",
        "class_idx = output.argmax().item()  # or specify class manually"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a77b5a4d",
      "metadata": {
        "id": "a77b5a4d"
      },
      "source": [
        "- We then reset the gradients in the model and perform a back propagation only for the neurons connected to the output class.\n",
        "- Resetting the gradients is necessary to avoid accumulating the gradients from the previous iteration.\n",
        "- Additionally, we only compute the gradients for the neurons connected to the output class in the final layer. This helps us identify the features that are important in predicting that particular class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b73c2de0",
      "metadata": {
        "id": "b73c2de0"
      },
      "outputs": [],
      "source": [
        "output = model(input_tensor)\n",
        "loss = output[0, class_idx]\n",
        "model.zero_grad()\n",
        "loss.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d8c2163",
      "metadata": {
        "id": "5d8c2163"
      },
      "source": [
        "#### 4.4 Compute GradCAM\n",
        "\n",
        "The gradients are of the format [B, C, H, W], where\n",
        "- B is the batch size (number of images- in our case, 1)\n",
        "- C is the number of channels/filters picked up by the layer. Each channel is a feature map.\n",
        "- H is the height of the feature map\n",
        "- W is the width of the feature map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c50b868",
      "metadata": {
        "id": "8c50b868"
      },
      "outputs": [],
      "source": [
        "pooled_grads = torch.mean(gradients, dim=(0, 2, 3))\n",
        "for i in range(activations.shape[1]):\n",
        "    activations[0, i, :, :] *= pooled_grads[i]\n",
        "\n",
        "gradcam = torch.mean(activations[0], dim=0).cpu().numpy()\n",
        "gradcam = np.maximum(gradcam, 0)\n",
        "gradcam /= gradcam.max()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5043e4b",
      "metadata": {
        "id": "e5043e4b"
      },
      "source": [
        "Upscale the GradCAM to the image resolution and scale its values to 0-255.\n",
        "\n",
        "Create a heatmap of the GradCAM values on the original image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3311dd25",
      "metadata": {
        "id": "3311dd25"
      },
      "outputs": [],
      "source": [
        "# Resize heatmap to original image size\n",
        "heatmap = cv2.resize(gradcam, (img.width, img.height))\n",
        "heatmap = np.uint8(255 * heatmap) # scale to 0-255 for colormap\n",
        "heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)  # apply colormap"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a652d434",
      "metadata": {
        "id": "a652d434"
      },
      "source": [
        "Convert the image to BGR and overlay the heatmap on top of it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e5a65cb",
      "metadata": {
        "id": "6e5a65cb"
      },
      "outputs": [],
      "source": [
        "img_np = np.array(img)\n",
        "img_np_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "# Overlay heatmap on top of the image\n",
        "overlay = cv2.addWeighted(img_np_bgr, 0.6, heatmap, 0.4, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ceb4387",
      "metadata": {
        "id": "3ceb4387"
      },
      "source": [
        "Finally, we can display the heatmap after converting the image back to RGB!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1debb055",
      "metadata": {
        "id": "1debb055"
      },
      "outputs": [],
      "source": [
        "# Convert back to RGB for matplotlib\n",
        "overlay_rgb = overlay[..., ::-1]\n",
        "\n",
        "plt.imshow(overlay_rgb)\n",
        "plt.axis('off')\n",
        "plt.title('Grad-CAM Heatmap Overlay')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bd2ff36",
      "metadata": {
        "id": "0bd2ff36"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}